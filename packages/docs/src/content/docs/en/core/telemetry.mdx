---
title: Langfuse telemetry
description: Enable and configure Langfuse telemetry for AI Kit agents
sidebar:
  order: 3
locale: en-US
---

import { PackageManagers } from 'starlight-package-managers'

AI Kit ships an optional integration with [Langfuse](https://langfuse.com/) to record `generate`/`stream` traces. This page explains how to install the OpenTelemetry dependencies, initialize the `LangfuseSpanProcessor`, and enable telemetry on your agents.

## Required dependencies

Add the Langfuse and OpenTelemetry packages. They are declared as peer dependencies in `@ai_kit/core` and must be installed in your application.

<PackageManagers pkg="@langfuse/otel @opentelemetry/sdk-trace-node" />

## Environment variables

Expose your Langfuse credentials on the server:

```bash
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
# Optional: US region
# LANGFUSE_BASE_URL=https://us.cloud.langfuse.com
```

## Initialize the Langfuse processor

Import `ensureLangfuseTelemetry` as early as possible (e.g. `instrumentation.ts`, server hook, or entry point). The function dynamically loads `@langfuse/otel`, registers a `NodeTracerProvider`, and returns a reusable handle.

```ts
// instrumentation.ts
import { ensureLangfuseTelemetry } from "@ai_kit/core";

const telemetry = await ensureLangfuseTelemetry({
  // Exclude spans generated by Next.js to keep only AI traffic.
  shouldExportSpan: ({ otelSpan }) =>
    otelSpan?.instrumentationScope?.name !== "next.js",
});

// In a serverless environment, force a flush at the end of the request.
export const flushLangfuse = () => telemetry.flush();
```

`ensureLangfuseTelemetry` throws a clear error if the optional dependencies are missing or if the environment keys are not set. The `autoFlush` mode defaults to `"process"`: `beforeExit`, `SIGINT`, and `SIGTERM` hooks automatically trigger `flush()`/`shutdown()` to avoid losing traces.

In a Next.js/Vercel handler, combine it with `after` to flush the buffers as soon as the response is sent:

```ts
import { after } from "next/server";
import { ensureLangfuseTelemetry } from "@ai_kit/core";

const telemetry = await ensureLangfuseTelemetry();

export async function POST(req: Request) {
  const result = await agent.stream({ /* ... */ });

  after(async () => {
    await telemetry.flush();
  });

  return result.toAIStreamResponse();
}
```

## Enable telemetry on an agent

Add `telemetry: true` when creating the agent to automatically set `experimental_telemetry.isEnabled = true` on each call.

```ts
import { Agent } from "@ai_kit/core";
import { openai } from "@ai-sdk/openai";

const supportAgent = new Agent({
  name: "support-assistant",
  model: openai("gpt-4.1-mini"),
  telemetry: true,
});
```

You can also toggle telemetry on the fly:

```ts
supportAgent.withTelemetry(false);
```

If you explicitly pass `experimental_telemetry.isEnabled = false` in a call, that choice wins and disables trace export, even if the agent is configured with `telemetry: true`.

## Customize telemetry per call

The `generate` and `stream` methods accept a new `telemetry` option. Its fields merge with `experimental_telemetry`:

```ts
await supportAgent.generate({
  prompt: "Create an incident ticket for user #42",
  telemetry: {
    functionId: "support-ticket",
    metadata: {
      ticketId: "42",
      priority: "high",
    },
    recordInputs: false,
  },
});
```

- `functionId`, `recordInputs`, and `recordOutputs` only apply if those values are not already provided through `experimental_telemetry`.
- `metadata` is shallow-merged: keys defined directly in `experimental_telemetry.metadata` stay in control.
- When the agent is not in telemetry mode, the overrides from `telemetry` are ignored, unless you enable `experimental_telemetry.isEnabled` yourself.

## Instrument workflows

Workflows now rely on the same OpenTelemetry stack. A simple `telemetry: true` in `createWorkflow` (or `workflow.withTelemetry(true)`) is enough to:

- name the root span after the workflow `id`;
- automatically record run inputs and outputs;
- expose the `input`, `output`, `metadata`, `name` attributes in addition to the `ai_kit.workflow.*` keys.

Pass an object if you want to customize `traceName`, add metadata, or disable the recording of sensitive data. Each run also produces:

- one span per step (`automatic` or `human`) with the `ai_kit.workflow.step.*` attributes;
- `human.requested` / `human.completed` events during manual interactions.

You can enrich a run on the fly by passing `telemetry: { metadata: { ... } }` to `workflow.run({ ... })`, or disable it temporarily with `telemetry: false`. Check the [`/en/core/workflows/telemetry`](/en/core/workflows/telemetry/) page for a full guide and Langfuse examples.

## Nitro/Nuxt example

Since `@ai_kit/core` v1.0.12, `ensureLangfuseTelemetry` automatically restores `addSpanProcessor`, which was removed by OpenTelemetry v2. You can therefore initialize Langfuse from a Nitro plugin without manual patching:

```ts
// server/plugins/langfuse-telemetry.ts
import { ensureLangfuseTelemetry } from "@ai_kit/core";

type LangfuseTelemetryHandle = Awaited<ReturnType<typeof ensureLangfuseTelemetry>>;

let telemetryPromise: Promise<LangfuseTelemetryHandle> | undefined;

const getTelemetry = () => {
  telemetryPromise ||= ensureLangfuseTelemetry({
    // Ignore internal Nuxt/Nitro spans to keep only your application traces.
    shouldExportSpan: ({ otelSpan }) => {
      const scopeName = otelSpan?.instrumentationScope?.name;
      return scopeName !== "nuxt" && scopeName !== "nitro";
    },
  });

  return telemetryPromise;
};

export const flushLangfuse = async () => {
  const telemetry = await getTelemetry();
  await telemetry.flush();
};

export default defineNitroPlugin(async (nitroApp) => {
  const telemetry = await getTelemetry();

  nitroApp.hooks.hook("close", async () => {
    await telemetry.shutdown();
  });

  nitroApp.hooks.hookOnce("request", async () => {
    await telemetry.flush();
  });
});
```
