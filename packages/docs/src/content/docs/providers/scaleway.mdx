---
title: Scaleway
description: Utilisation du provider Scaleway avec AI Kit
sidebar:
  order: 1
---

import { PackageManagers } from 'starlight-package-managers'

Le provider Scaleway permet d'utiliser les modèles LLM hébergés sur la plateforme [Scaleway AI](https://www.scaleway.com/fr/ai/) avec AI Kit. Il s'appuie sur l'API compatible OpenAI de Scaleway.

## Installation

Installez les dépendances nécessaires :

<PackageManagers pkg="@ai_kit/core @ai-sdk/openai ai" />

## Configuration

Configurez votre clé API Scaleway dans une variable d'environnement :

```bash
export SCALEWAY_API_KEY="skw-..."
```

Vous pouvez obtenir votre clé API depuis la [console Scaleway](https://console.scaleway.com/iam/api-keys).

## Utilisation avec un Agent

Importez le provider `scaleway` et utilisez-le pour créer un agent :

```ts
import { Agent, scaleway } from "@ai_kit/core";

const assistant = new Agent({
  name: "assistant-scaleway",
  instructions: "Tu es un assistant utile et précis.",
  model: scaleway("gpt-oss-120b"),
});

const result = await assistant.generate({
  prompt: "Quelle est la capitale de la France ?",
});

console.log(result.text);
```

## Modèles disponibles

Scaleway propose plusieurs modèles LLM accessibles via le provider :

| Modèle | Description | Taille |
|--------|-------------|--------|
| `gpt-oss-120b` | Modèle généraliste performant | 120B |
| `llama-3.3-70b-instruct` | Meta Llama 3.3 optimisé pour l'instruction | 70B |
| `llama-3.1-8b-instruct` | Meta Llama 3.1 compact | 8B |
| `mistral-small-3.2-24b-instruct-2506` | Mistral Small dernière génération | 24B |
| `mistral-nemo-instruct-2407` | Mistral Nemo optimisé | 12B |
| `qwen3-235b-a22b-instruct-2507` | Qwen 3 grande capacité | 235B |
| `qwen3-coder-30b-a3b-instruct` | Qwen 3 spécialisé code | 30B |
| `deepseek-r1-distill-llama-70b` | DeepSeek R1 distillé | 70B |
| `gemma-3-27b-it` | Google Gemma 3 | 27B |
| `voxtral-small-24b-2507` | Voxtral Small | 24B |
| `devstral-small-2505` | Devstral Small pour le développement | 25B |
| `pixtral-12b-2409` | Pixtral multimodal | 12B |

## Exemples d'utilisation

### Générer une réponse structurée

```ts
import { Agent, scaleway, Output } from "@ai_kit/core";
import { z } from "zod";

const codeSchema = Output.object({
  schema: z.object({
    language: z.string(),
    code: z.string(),
    explanation: z.string(),
  }),
});

const assistant = new Agent({
  name: "code-assistant",
  instructions: "Tu es un expert en programmation.",
  model: scaleway("qwen3-coder-30b-a3b-instruct"),
});

const result = await assistant.generate({
  prompt: "Écris une fonction Python pour calculer la suite de Fibonacci",
  structuredOutput: codeSchema,
});

console.log(result.experimental_output);
```

### Utiliser dans un workflow

```ts
import { Agent, createStep, createWorkflow, scaleway } from "@ai_kit/core";
import { z } from "zod";

const assistant = new Agent({
  name: "analyzer",
  instructions: "Analyse le sentiment du texte.",
  model: scaleway("mistral-small-3.2-24b-instruct-2506"),
});

const analyzeStep = createStep({
  id: "analyze-text",
  inputSchema: z.object({ text: z.string() }),
  handler: async ({ input }) => {
    const result = await assistant.generate({
      prompt: `Analyse le sentiment de ce texte : "${input.text}"`,
    });

    return { sentiment: result.text };
  },
});

const workflow = createWorkflow({
  id: "sentiment-analysis",
  inputSchema: z.object({ text: z.string() }),
  outputSchema: z.object({ sentiment: z.string() }),
})
  .then(analyzeStep)
  .commit();
```

### Streaming de réponses

```ts
const assistant = new Agent({
  name: "assistant-stream",
  instructions: "Tu es un assistant créatif.",
  model: scaleway("llama-3.3-70b-instruct"),
});

const stream = await assistant.stream({
  prompt: "Raconte-moi une histoire courte sur un robot qui apprend à cuisiner.",
  temperature: 0.7,
});

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}
```

## Choisir le bon modèle

- **Pour la génération de code** : `qwen3-coder-30b-a3b-instruct`, `devstral-small-2505`
- **Pour les tâches générales** : `gpt-oss-120b`, `llama-3.3-70b-instruct`
- **Pour les applications légères** : `llama-3.1-8b-instruct`, `mistral-nemo-instruct-2407`
- **Pour le raisonnement complexe** : `deepseek-r1-distill-llama-70b`, `qwen3-235b-a22b-instruct-2507`
- **Pour le multimodal** : `pixtral-12b-2409`

## Bonnes pratiques

1. **Gestion des clés API** : Ne committez jamais votre clé API dans le code. Utilisez des variables d'environnement ou un gestionnaire de secrets.

2. **Gestion des erreurs** : Implémentez une gestion des erreurs appropriée pour les appels API :

```ts
try {
  const result = await assistant.generate({ prompt: "..." });
  console.log(result.text);
} catch (error) {
  console.error("Erreur lors de la génération :", error);
}
```

3. **Limites de tokens** : Surveillez l'usage avec `maxOutputTokens` pour contrôler les coûts :

```ts
const result = await assistant.generate({
  prompt: "...",
  maxOutputTokens: 500,
});
```

4. **Temperature et créativité** : Ajustez la température selon vos besoins :
   - `0.0-0.3` : réponses déterministes et précises
   - `0.4-0.7` : équilibre créativité/cohérence
   - `0.8-1.0` : réponses très créatives

## Ressources

- [Documentation Scaleway AI](https://www.scaleway.com/fr/docs/ai-data/)
- [Console Scaleway](https://console.scaleway.com/)
- [Tarification Scaleway AI](https://www.scaleway.com/fr/pricing/#ai-data)
