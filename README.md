# AI Kit

AI Kit est le micro-framework agentique interne dâ€™Aidalinfo. Son objectif est dâ€™offrir une couche dâ€™abstraction stable au-dessus de lâ€™AI SDK et des providers LLM, afin dâ€™Ã©viter la volatilitÃ© que nous avons constatÃ©e avec Mastra ou VoltAgent. La base de code est volontairement compacte, lisible et orientÃ©e vers les cas dâ€™usage que nous maÃ®trisons en production.

## Pourquoi AI KitÂ ?

- **StabilitÃ©**Â : nous gardons la main sur les Ã©volutions critiques sans subir les breaking changes des frameworks externes.
- **ClartÃ©**Â : la structure du code privilÃ©gie des concepts simples (agents, chunking, workflows) et documentÃ©s.
- **InteropÃ©rabilitÃ©**Â : AI Kit reste compatible avec les providers de lâ€™Ã©cosystÃ¨me AI SDK tout en ajoutant notre logique mÃ©tier.

## Getting Started

Installez directement le package `@ai_kit/core` depuis ce dÃ©pÃ´tÂ :

```bash
npm i @ai_kit/core
```

Configurez vos clÃ©s API (ex. Scaleway) dans lâ€™environnementÂ :

```bash
export SCALEWAY_API_KEY="skw-..."
```

## Serveur

Besoin dâ€™une API prÃªte Ã  lâ€™emploi pour hÃ©berger vos agents et workflowsÂ ? Le package [`@ai_kit/server`](https://www.npmjs.com/package/@ai_kit/server) fournit un serveur HTTP minimal dÃ©jÃ  cÃ¢blÃ© avec AI Kit (routing JSON, gestion des secrets, hooks de tÃ©lÃ©mÃ©trie).

```bash
npm i @ai_kit/server
```

Consultez la page npm pour les exemples de configuration dÃ©taillÃ©s et les options dâ€™exÃ©cution.

## Utilisation rapide

### CrÃ©er un agent

```ts
import { Agent, scaleway } from "@ai_kit/core";

const assistant = new Agent({
  name: "assistant-internal",
  instructions: "Tu aides les Ã©quipes Aidalinfo.",
  model: scaleway("gpt-oss-120b"),
});

const result = await assistant.generate({
  prompt: "Donne trois conseils pour intÃ©grer AI Kit." ,
});

console.log(result.text);
```

ðŸ‘‰ Guides dÃ©taillÃ©sÂ : [Agents](https://ai.aidalinfo.fr/fr/agents)

### Activer la tÃ©lÃ©mÃ©trie Langfuse

```ts
import { ensureLangfuseTelemetry, Agent, createWorkflow, createStep } from "@ai_kit/core";
import { openai } from "@ai-sdk/openai";

await ensureLangfuseTelemetry(); // enregistre le LangfuseSpanProcessor

const agent = new Agent({
  name: "support",
  model: openai("gpt-4.1-mini"),
  telemetry: true,
});

const workflow = createWorkflow({ id: "demo", telemetry: true })
  .then(createStep({ id: "noop", handler: ({ input }) => input }))
  .commit();

await workflow.run({
  inputData: { foo: "bar" },
  telemetry: { metadata: { requestId: "run_123" } },
});
```

ðŸ‘‰ Configuration complÃ¨teÂ : [TÃ©lÃ©mÃ©trie Langfuse](https://ai.aidalinfo.fr/fr/telemetrie/langfuse)

### DÃ©couper du contenu

```ts
import { splitTextRecursively } from "@ai_kit/core";

const chunks = splitTextRecursively(longRapport, {
  chunkSize: 400,
  chunkOverlap: 40,
});

// Exemple : crÃ©er un index de recherche
const passages = chunks.map((chunk) => ({
  id: `rapport-${chunk.index}`,
  text: chunk.content,
  start: chunk.start,
  end: chunk.end,
}));
```

ðŸ‘‰ Plus dâ€™exemplesÂ : [Chunks](https://ai.aidalinfo.fr/fr/utils/chunking)

### Orchestrer un workflow

```ts
import { createStep, createWorkflow } from "@ai_kit/core";

const enrichData = createStep({
  id: "enrich-data",
  handler: async ({ input }) => ({ ...input, enriched: true }),
});

const pipeline = createWorkflow({ id: "sample" })
  .then(enrichData)
  .commit();

const outcome = await pipeline.run({ inputData: { id: "123" } });
console.log(outcome.result);
```

ðŸ‘‰ Documentation complÃ¨teÂ : [Workflows](https://ai.aidalinfo.fr/fr/workflows)

## Structure du dÃ©pÃ´t

- `packages/core`Â : cÅ“ur du framework (agents, chunking, workflows, providers).
- `docs/core`Â : documentation interne (agents, chunks, workflows, bonnes pratiques).
- `packages/mcp-docs-server`Â : serveur MCP exposant la documentation AI Kit.

## Serveur MCP Docs

Le package `@ai_kit/mcp-docs` fournit un serveur MCP qui diffuse toute la documentation (rÃ©pertoire `docs/`, `README.md` racine, et les README de packages si prÃ©sents). Deux outils sont exposÃ©sÂ :

- `ai_kit-docs`Â : navigation dans lâ€™arborescence, lecture des fichiers et survol des mots-clÃ©s.
- `ai_kit-docs-search`Â : recherche plein texte avec extraits contextualisÃ©s.

### Utilisation rapide

```bash
npx -y @ai_kit/mcp-docs
```

Le serveur Ã©coute en STDIO. Tu peux le brancher directement dans nâ€™importe quel client MCP (Claude Desktop, Cursor, etc.).

### Exemple de configuration Claude Desktop

```json
{
  "mcpServers": {
    "ai_kit-docs": {
      "command": "npx",
      "args": ["-y", "@ai_kit/mcp-docs"],
    }
  },
}
```

### Inclure dâ€™autres README

Le script de build copie automatiquement :

- `docs/**` â†’ `dist/docs/**` ;
- `README.md` (racine) â†’ `dist/docs/README.md` ;
- `packages/<nom>/README.md` â†’ `dist/docs/<nom>/README.md` (si le fichier existe).

Pour exposer le README du package `@ai_kit/core`, il suffit donc de crÃ©er `packages/core/README.md`. Le prochain `pnpm --filter @ai_kit/mcp-docs build` rÃ©pliquera ce fichier et il deviendra accessible via lâ€™outil MCP.

## SDK MCP

Le package `@ai_kit/mcp` fournit un mini DSL inspirÃ© de Mastra pour dÃ©clarer des serveurs MCP sans recourir directement au SDK brut. Il gÃ¨re lâ€™enregistrement des outils, ressources et prompts, et convertit automatiquement les retours simples (`string`, tableaux de textes, buffers) au format attendu par le protocole.

### Exemple rapide

```ts
import { defineMcpServer, defineTool } from "@ai_kit/mcp";
import { z } from "zod";

const server = defineMcpServer({
  name: "ai-kit-lab",
  version: "0.1.0",
  tools: {
    ping: defineTool({
      description: "Renvoie un ping lisible par un humain.",
      inputSchema: z.object({ message: z.string() }),
      handler: async ({ message }) => `pong: ${message}`
    })
  }
});

await server.startStdioServer({
  onReady: () => {
    console.error("Serveur MCP ai-kit-lab prÃªt sur stdio");
  }
});
```

Le DSL accepte Ã©galement des ressources (fichiers statiques ou dynamiques via templates) et des prompts. Les fonctions `defineTool`, `defineResource` et `definePrompt` sont optionnellesÂ : un simple objet JavaScript suffit, elles servent surtout Ã  la complÃ©tion TypeScript.

## Contribuer

- Respectez la structure existante et les patterns introduits (ex. `TChunkDocument`, `createWorkflow`).
- Ajoutez des tests Vitest lorsque vous livrez une fonctionnalitÃ© critique.
- Documentez vos ajouts dans `docs/core` si vous introduisez un nouveau concept ou un flux important.

## Ressources

- [Documentation Agents](https://ai.aidalinfo.fr/fr/agents)
- [Documentation Chunks](https://ai.aidalinfo.fr/fr/utils/chunking)
- [Documentation Workflows](https://ai.aidalinfo.fr/fr/workflows)
- [Documentation MCP](https://ai.aidalinfo.fr/fr/mcp/usage)

Lâ€™objectif est de renforcer notre autonomie technique autour des assistants et pipelines AIÂ : nâ€™hÃ©sitez pas Ã  complÃ©ter ces ressources et Ã  proposer des amÃ©liorations.
